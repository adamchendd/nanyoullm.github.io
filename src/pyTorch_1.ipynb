{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "- 这里我们学习一下pyTorch的Tensors基本数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.实现一个简单的神经网络反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 100\n",
    "# 输入维度\n",
    "input_size = 64\n",
    "# 隐层维度\n",
    "hidden_size = 1000\n",
    "# 输出维度\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 正态分布随机定义训练数据\n",
    "# type可以将tensor转换成指定的数据格式\n",
    "x = torch.randn(batch_size, input_size).type(torch.FloatTensor)\n",
    "y = torch.randn(batch_size, output_size).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可训练参数定义\n",
    "w1 = torch.randn(input_size, hidden_size)\n",
    "w2 = torch.randn(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们看一下反向传播的计算步骤，图片截自课程[UFLDL](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)\n",
    "\n",
    "![反向传播计算步骤](https://github.com/nanyoullm/nanyoullm.github.io/blob/master/img/backpro.png?raw=true)\n",
    "<br\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 36958734.10462731\n",
      "step: 1, loss: 36420465.53241235\n",
      "step: 2, loss: 41324088.96093157\n",
      "step: 3, loss: 50436694.73321782\n",
      "step: 4, loss: 62901233.96598019\n",
      "step: 5, loss: 77497475.02231407\n",
      "step: 6, loss: 91933759.40653643\n",
      "step: 7, loss: 103017011.3388789\n",
      "step: 8, loss: 107079230.78604555\n",
      "step: 9, loss: 102112495.1021345\n",
      "step: 10, loss: 88753806.65675187\n",
      "step: 11, loss: 70579758.22284675\n",
      "step: 12, loss: 51928516.71890558\n",
      "step: 13, loss: 35959434.3580873\n",
      "step: 14, loss: 23853022.111877263\n",
      "step: 15, loss: 15420653.957485316\n",
      "step: 16, loss: 9848866.04301719\n",
      "step: 17, loss: 6291443.947651656\n",
      "step: 18, loss: 4060000.1642449554\n",
      "step: 19, loss: 2672350.366183058\n",
      "step: 20, loss: 1809777.4696968\n",
      "step: 21, loss: 1270769.3968613164\n",
      "step: 22, loss: 929764.6189778342\n",
      "step: 23, loss: 710120.4443849543\n",
      "step: 24, loss: 565077.3844164119\n",
      "step: 25, loss: 466229.23316568375\n",
      "step: 26, loss: 396269.8328382359\n",
      "step: 27, loss: 344723.6662716504\n",
      "step: 28, loss: 305095.22214080935\n",
      "step: 29, loss: 273451.1213553838\n",
      "step: 30, loss: 247292.10638130494\n",
      "step: 31, loss: 225068.93985607618\n",
      "step: 32, loss: 205789.19409774337\n",
      "step: 33, loss: 188766.77542199154\n",
      "step: 34, loss: 173577.3251740167\n",
      "step: 35, loss: 159899.3783165165\n",
      "step: 36, loss: 147507.66702290246\n",
      "step: 37, loss: 136227.80645482044\n",
      "step: 38, loss: 125929.86110890727\n",
      "step: 39, loss: 116502.91045968852\n",
      "step: 40, loss: 107855.56200813502\n",
      "step: 41, loss: 99911.68486616675\n",
      "step: 42, loss: 92602.44989131819\n",
      "step: 43, loss: 85869.42174789289\n",
      "step: 44, loss: 79663.88793107733\n",
      "step: 45, loss: 73936.29401812868\n",
      "step: 46, loss: 68649.34571448772\n",
      "step: 47, loss: 63765.9946561989\n",
      "step: 48, loss: 59252.65578991198\n",
      "step: 49, loss: 55076.32167747503\n",
      "step: 50, loss: 51214.15524622191\n",
      "step: 51, loss: 47637.87054209178\n",
      "step: 52, loss: 44324.255311585846\n",
      "step: 53, loss: 41255.20001963731\n",
      "step: 54, loss: 38408.657999399846\n",
      "step: 55, loss: 35768.454924347985\n",
      "step: 56, loss: 33318.41146447255\n",
      "step: 57, loss: 31045.171736803037\n",
      "step: 58, loss: 28932.898292487997\n",
      "step: 59, loss: 26971.294211561522\n",
      "step: 60, loss: 25150.20167828843\n",
      "step: 61, loss: 23455.843354226337\n",
      "step: 62, loss: 21880.796509497137\n",
      "step: 63, loss: 20415.730448547147\n",
      "step: 64, loss: 19051.82500420668\n",
      "step: 65, loss: 17783.52446604117\n",
      "step: 66, loss: 16602.858412709\n",
      "step: 67, loss: 15502.969278975634\n",
      "step: 68, loss: 14479.756183579455\n",
      "step: 69, loss: 13525.409428136076\n",
      "step: 70, loss: 12637.32543526713\n",
      "step: 71, loss: 11808.83309057384\n",
      "step: 72, loss: 11036.979452452182\n",
      "step: 73, loss: 10317.25769966351\n",
      "step: 74, loss: 9645.827426044245\n",
      "step: 75, loss: 9019.710357708214\n",
      "step: 76, loss: 8435.64657312745\n",
      "step: 77, loss: 7890.3565379215215\n",
      "step: 78, loss: 7381.534511033777\n",
      "step: 79, loss: 6906.464358042367\n",
      "step: 80, loss: 6463.162555730491\n",
      "step: 81, loss: 6048.873335502334\n",
      "step: 82, loss: 5662.0764646840535\n",
      "step: 83, loss: 5300.6138002989\n",
      "step: 84, loss: 4963.30396340058\n",
      "step: 85, loss: 4647.57549226049\n",
      "step: 86, loss: 4352.859740672032\n",
      "step: 87, loss: 4077.287341683295\n",
      "step: 88, loss: 3819.629162730835\n",
      "step: 89, loss: 3578.64549636202\n",
      "step: 90, loss: 3353.3992652331544\n",
      "step: 91, loss: 3142.50927985486\n",
      "step: 92, loss: 2945.3650866750554\n",
      "step: 93, loss: 2760.8205420833347\n",
      "step: 94, loss: 2588.1289540562625\n",
      "step: 95, loss: 2426.529089736423\n",
      "step: 96, loss: 2275.3421025324246\n",
      "step: 97, loss: 2133.7022786708626\n",
      "step: 98, loss: 2001.15316289999\n",
      "step: 99, loss: 1877.026358877191\n",
      "step: 100, loss: 1760.7902036010437\n",
      "step: 101, loss: 1651.8650595570157\n",
      "step: 102, loss: 1549.905351903407\n",
      "step: 103, loss: 1454.3277667855946\n",
      "step: 104, loss: 1364.8358360711754\n",
      "step: 105, loss: 1280.8985469711824\n",
      "step: 106, loss: 1202.3176327198976\n",
      "step: 107, loss: 1128.605478707701\n",
      "step: 108, loss: 1059.548619093057\n",
      "step: 109, loss: 994.7491669381957\n",
      "step: 110, loss: 933.9946473192914\n",
      "step: 111, loss: 877.0911566330988\n",
      "step: 112, loss: 823.6742251801086\n",
      "step: 113, loss: 773.5966495481854\n",
      "step: 114, loss: 726.6605588195811\n",
      "step: 115, loss: 682.6110905818601\n",
      "step: 116, loss: 641.298588709391\n",
      "step: 117, loss: 602.5140639440915\n",
      "step: 118, loss: 566.1567803880737\n",
      "step: 119, loss: 532.0028280344084\n",
      "step: 120, loss: 499.9822632228463\n",
      "step: 121, loss: 469.91219295496063\n",
      "step: 122, loss: 441.67923549533424\n",
      "step: 123, loss: 415.18821701935144\n",
      "step: 124, loss: 390.30816500564134\n",
      "step: 125, loss: 366.9541800748726\n",
      "step: 126, loss: 345.0093647628855\n",
      "step: 127, loss: 324.41412964237816\n",
      "step: 128, loss: 305.0718339691027\n",
      "step: 129, loss: 286.88456234155217\n",
      "step: 130, loss: 269.8252479476173\n",
      "step: 131, loss: 253.78423326952972\n",
      "step: 132, loss: 238.722541603434\n",
      "step: 133, loss: 224.5573473539042\n",
      "step: 134, loss: 211.2592138611748\n",
      "step: 135, loss: 198.76180460660385\n",
      "step: 136, loss: 187.01821005911464\n",
      "step: 137, loss: 175.97343631374957\n",
      "step: 138, loss: 165.59831601716985\n",
      "step: 139, loss: 155.84500670265925\n",
      "step: 140, loss: 146.67164373235573\n",
      "step: 141, loss: 138.05311817993672\n",
      "step: 142, loss: 129.94486395303613\n",
      "step: 143, loss: 122.32740887465957\n",
      "step: 144, loss: 115.1550589852041\n",
      "step: 145, loss: 108.4164645612629\n",
      "step: 146, loss: 102.07955909813072\n",
      "step: 147, loss: 96.11632530146068\n",
      "step: 148, loss: 90.51063345167114\n",
      "step: 149, loss: 85.23396789139093\n",
      "step: 150, loss: 80.27197445032368\n",
      "step: 151, loss: 75.6030226086143\n",
      "step: 152, loss: 71.20980953726928\n",
      "step: 153, loss: 67.07634186825393\n",
      "step: 154, loss: 63.186452593138334\n",
      "step: 155, loss: 59.52682894210113\n",
      "step: 156, loss: 56.07968870314221\n",
      "step: 157, loss: 52.8379928584766\n",
      "step: 158, loss: 49.78649895027603\n",
      "step: 159, loss: 46.91230086971315\n",
      "step: 160, loss: 44.207615312803966\n",
      "step: 161, loss: 41.663505838969854\n",
      "step: 162, loss: 39.262861503183615\n",
      "step: 163, loss: 37.00654763139937\n",
      "step: 164, loss: 34.8799220436872\n",
      "step: 165, loss: 32.87942481046805\n",
      "step: 166, loss: 30.993570953924873\n",
      "step: 167, loss: 29.218764397781456\n",
      "step: 168, loss: 27.546451816608283\n",
      "step: 169, loss: 25.97202694061042\n",
      "step: 170, loss: 24.48784738377791\n",
      "step: 171, loss: 23.09083122965518\n",
      "step: 172, loss: 21.774902062201846\n",
      "step: 173, loss: 20.533341497241803\n",
      "step: 174, loss: 19.364874162153612\n",
      "step: 175, loss: 18.26364169106147\n",
      "step: 176, loss: 17.225692096271892\n",
      "step: 177, loss: 16.24864049571568\n",
      "step: 178, loss: 15.326786204265218\n",
      "step: 179, loss: 14.457170383320772\n",
      "step: 180, loss: 13.639395088353112\n",
      "step: 181, loss: 12.867450172054745\n",
      "step: 182, loss: 12.139609988472998\n",
      "step: 183, loss: 11.453902234842452\n",
      "step: 184, loss: 10.807670697772146\n",
      "step: 185, loss: 10.197819270827374\n",
      "step: 186, loss: 9.62367056769471\n",
      "step: 187, loss: 9.081104317414464\n",
      "step: 188, loss: 8.57067913294983\n",
      "step: 189, loss: 8.088799780439714\n",
      "step: 190, loss: 7.63459141323067\n",
      "step: 191, loss: 7.205706195410212\n",
      "step: 192, loss: 6.802236655062636\n",
      "step: 193, loss: 6.420848582124702\n",
      "step: 194, loss: 6.061298302741147\n",
      "step: 195, loss: 5.721973859783235\n",
      "step: 196, loss: 5.402394007457007\n",
      "step: 197, loss: 5.100787687825115\n",
      "step: 198, loss: 4.815719165840116\n",
      "step: 199, loss: 4.547352913492256\n",
      "step: 200, loss: 4.293946662280369\n",
      "step: 201, loss: 4.0545224177553845\n",
      "step: 202, loss: 3.8288339048922957\n",
      "step: 203, loss: 3.615933554643254\n",
      "step: 204, loss: 3.415230453360703\n",
      "step: 205, loss: 3.225369602263788\n",
      "step: 206, loss: 3.0465641563348758\n",
      "step: 207, loss: 2.8777311445221594\n",
      "step: 208, loss: 2.7183131402140264\n",
      "step: 209, loss: 2.567790900489503\n",
      "step: 210, loss: 2.4258683314189717\n",
      "step: 211, loss: 2.291345368516989\n",
      "step: 212, loss: 2.165139391015021\n",
      "step: 213, loss: 2.0454026396653298\n",
      "step: 214, loss: 1.9327078411987095\n",
      "step: 215, loss: 1.8261859937615377\n",
      "step: 216, loss: 1.7255945207049246\n",
      "step: 217, loss: 1.6303668963574935\n",
      "step: 218, loss: 1.5408021622113708\n",
      "step: 219, loss: 1.456085290056965\n",
      "step: 220, loss: 1.3761829747152774\n",
      "step: 221, loss: 1.3005488528468767\n",
      "step: 222, loss: 1.2292051629089018\n",
      "step: 223, loss: 1.1617250188360773\n",
      "step: 224, loss: 1.0981284027521134\n",
      "step: 225, loss: 1.0379928433979535\n",
      "step: 226, loss: 0.9812254960220612\n",
      "step: 227, loss: 0.9275933737889511\n",
      "step: 228, loss: 0.876885829178135\n",
      "step: 229, loss: 0.8289712839399019\n",
      "step: 230, loss: 0.7838411548242676\n",
      "step: 231, loss: 0.7409834379379648\n",
      "step: 232, loss: 0.7005953277463179\n",
      "step: 233, loss: 0.6624020648651096\n",
      "step: 234, loss: 0.6263154347813478\n",
      "step: 235, loss: 0.5923216529618931\n",
      "step: 236, loss: 0.560174474341367\n",
      "step: 237, loss: 0.5297664092738259\n",
      "step: 238, loss: 0.5009287335275161\n",
      "step: 239, loss: 0.47373100269867074\n",
      "step: 240, loss: 0.44807582469945917\n",
      "step: 241, loss: 0.4237496623377741\n",
      "step: 242, loss: 0.40077277166745984\n",
      "step: 243, loss: 0.3791110898290697\n",
      "step: 244, loss: 0.35857764886055166\n",
      "step: 245, loss: 0.3392031517175653\n",
      "step: 246, loss: 0.32093869416847154\n",
      "step: 247, loss: 0.3035922447472239\n",
      "step: 248, loss: 0.28719957503898685\n",
      "step: 249, loss: 0.2716915108670348\n",
      "step: 250, loss: 0.25703308543290815\n",
      "step: 251, loss: 0.24322699299171124\n",
      "step: 252, loss: 0.23010668168563564\n",
      "step: 253, loss: 0.21778442033740952\n",
      "step: 254, loss: 0.20602920634147837\n",
      "step: 255, loss: 0.1950240482240695\n",
      "step: 256, loss: 0.18445535531371626\n",
      "step: 257, loss: 0.17456610351598667\n",
      "step: 258, loss: 0.16516324130168947\n",
      "step: 259, loss: 0.15639863406751875\n",
      "step: 260, loss: 0.1480066617767033\n",
      "step: 261, loss: 0.1400654865466544\n",
      "step: 262, loss: 0.13259516998196297\n",
      "step: 263, loss: 0.1255201309074787\n",
      "step: 264, loss: 0.11878742957208055\n",
      "step: 265, loss: 0.11241687960772495\n",
      "step: 266, loss: 0.10642040450248302\n",
      "step: 267, loss: 0.10071760142586683\n",
      "step: 268, loss: 0.09534516448640695\n",
      "step: 269, loss: 0.090237473764603\n",
      "step: 270, loss: 0.08547598562648462\n",
      "step: 271, loss: 0.08089084644530264\n",
      "step: 272, loss: 0.07658874113895164\n",
      "step: 273, loss: 0.07250427325048436\n",
      "step: 274, loss: 0.06867050250643136\n",
      "step: 275, loss: 0.0650339997457341\n",
      "step: 276, loss: 0.0615570978218154\n",
      "step: 277, loss: 0.05830076939563633\n",
      "step: 278, loss: 0.05518634067818695\n",
      "step: 279, loss: 0.052251040128143744\n",
      "step: 280, loss: 0.04951025063916914\n",
      "step: 281, loss: 0.046904262655629525\n",
      "step: 282, loss: 0.0443926497716971\n",
      "step: 283, loss: 0.04206571840063411\n",
      "step: 284, loss: 0.039825448745204106\n",
      "step: 285, loss: 0.03772234313262057\n",
      "step: 286, loss: 0.035752555476930015\n",
      "step: 287, loss: 0.03384057828574871\n",
      "step: 288, loss: 0.032079333874340366\n",
      "step: 289, loss: 0.03037724072389847\n",
      "step: 290, loss: 0.028780633922345977\n",
      "step: 291, loss: 0.02727247923087117\n",
      "step: 292, loss: 0.025827733531052832\n",
      "step: 293, loss: 0.024473193818359606\n",
      "step: 294, loss: 0.02319537617487627\n",
      "step: 295, loss: 0.02196634599670655\n",
      "step: 296, loss: 0.02081217097714544\n",
      "step: 297, loss: 0.01973820738154758\n",
      "step: 298, loss: 0.01870555245185984\n",
      "step: 299, loss: 0.017722999197090195\n",
      "step: 300, loss: 0.01679098433980003\n",
      "step: 301, loss: 0.01590765921072984\n",
      "step: 302, loss: 0.015083300291544366\n",
      "step: 303, loss: 0.014292933792579579\n",
      "step: 304, loss: 0.013548939914392322\n",
      "step: 305, loss: 0.0128465836454882\n",
      "step: 306, loss: 0.012175025764466066\n",
      "step: 307, loss: 0.011548062098404865\n",
      "step: 308, loss: 0.010940396103086836\n",
      "step: 309, loss: 0.01037131034223282\n",
      "step: 310, loss: 0.009831872778804929\n",
      "step: 311, loss: 0.009335680148877534\n",
      "step: 312, loss: 0.008852438752946112\n",
      "step: 313, loss: 0.008399456224589374\n",
      "step: 314, loss: 0.007965030966950865\n",
      "step: 315, loss: 0.007557524740493218\n",
      "step: 316, loss: 0.007172992325246003\n",
      "step: 317, loss: 0.006802737795908859\n",
      "step: 318, loss: 0.0064574659142296875\n",
      "step: 319, loss: 0.006127777838969756\n",
      "step: 320, loss: 0.005816744934635996\n",
      "step: 321, loss: 0.005519987350291661\n",
      "step: 322, loss: 0.005240457738360585\n",
      "step: 323, loss: 0.00497404133081536\n",
      "step: 324, loss: 0.004716310282703324\n",
      "step: 325, loss: 0.004483728986368743\n",
      "step: 326, loss: 0.004261180029085168\n",
      "step: 327, loss: 0.004048623764414172\n",
      "step: 328, loss: 0.0038438611482846247\n",
      "step: 329, loss: 0.0036600124498057385\n",
      "step: 330, loss: 0.0034800579270126697\n",
      "step: 331, loss: 0.0033094662436439354\n",
      "step: 332, loss: 0.003149417828380463\n",
      "step: 333, loss: 0.0029955029555056717\n",
      "step: 334, loss: 0.0028476900223665114\n",
      "step: 335, loss: 0.002711632938141653\n",
      "step: 336, loss: 0.002587294299924714\n",
      "step: 337, loss: 0.002461427212666137\n",
      "step: 338, loss: 0.002343077196255039\n",
      "step: 339, loss: 0.002236621313034204\n",
      "step: 340, loss: 0.0021279056257900303\n",
      "step: 341, loss: 0.0020274404850928462\n",
      "step: 342, loss: 0.001933297558950664\n",
      "step: 343, loss: 0.0018444278427076632\n",
      "step: 344, loss: 0.0017601868884557087\n",
      "step: 345, loss: 0.0016824707452564591\n",
      "step: 346, loss: 0.0016100865492099797\n",
      "step: 347, loss: 0.001538418991205548\n",
      "step: 348, loss: 0.0014682945797630945\n",
      "step: 349, loss: 0.0014011701854430292\n",
      "step: 350, loss: 0.0013399263163305364\n",
      "step: 351, loss: 0.001281130653245463\n",
      "step: 352, loss: 0.0012255977177660221\n",
      "step: 353, loss: 0.0011743289736748687\n",
      "step: 354, loss: 0.0011261056291568435\n",
      "step: 355, loss: 0.001075057467422036\n",
      "step: 356, loss: 0.001033930778413128\n",
      "step: 357, loss: 0.0009892376149684343\n",
      "step: 358, loss: 0.0009476339981491673\n",
      "step: 359, loss: 0.0009106599517150205\n",
      "step: 360, loss: 0.0008740583287354144\n",
      "step: 361, loss: 0.0008383121505635138\n",
      "step: 362, loss: 0.0008031829989364747\n",
      "step: 363, loss: 0.0007702155885666064\n",
      "step: 364, loss: 0.0007410781114224781\n",
      "step: 365, loss: 0.0007128495586105517\n",
      "step: 366, loss: 0.0006853747495929874\n",
      "step: 367, loss: 0.0006609481079652459\n",
      "step: 368, loss: 0.00063518748113775\n",
      "step: 369, loss: 0.0006107598160700507\n",
      "step: 370, loss: 0.0005903512708350551\n",
      "step: 371, loss: 0.0005666660848394761\n",
      "step: 372, loss: 0.0005483391502074178\n",
      "step: 373, loss: 0.0005288958223531315\n",
      "step: 374, loss: 0.000507983906136631\n",
      "step: 375, loss: 0.000490450067072018\n",
      "step: 376, loss: 0.0004738475646939022\n",
      "step: 377, loss: 0.00045675421503929906\n",
      "step: 378, loss: 0.00044127886200959546\n",
      "step: 379, loss: 0.0004268001740507926\n",
      "step: 380, loss: 0.00041093066959344005\n",
      "step: 381, loss: 0.0003989803108353854\n",
      "step: 382, loss: 0.00038580427930232086\n",
      "step: 383, loss: 0.00037323952912510575\n",
      "step: 384, loss: 0.00036150745103560866\n",
      "step: 385, loss: 0.00035042536614762754\n",
      "step: 386, loss: 0.00033890400205648097\n",
      "step: 387, loss: 0.0003287496898710107\n",
      "step: 388, loss: 0.0003180000726511617\n",
      "step: 389, loss: 0.00030966032922534817\n",
      "step: 390, loss: 0.0003006532519779781\n",
      "step: 391, loss: 0.00029137528649123\n",
      "step: 392, loss: 0.000282393343331442\n",
      "step: 393, loss: 0.0002740155236251908\n",
      "step: 394, loss: 0.00026668450334724203\n",
      "step: 395, loss: 0.0002599249532852371\n",
      "step: 396, loss: 0.0002524026786349656\n",
      "step: 397, loss: 0.00024456223645766273\n",
      "step: 398, loss: 0.00023755174579348417\n",
      "step: 399, loss: 0.000231883030032895\n",
      "step: 400, loss: 0.0002246158837723054\n",
      "step: 401, loss: 0.00021914898323001364\n",
      "step: 402, loss: 0.0002130887977843804\n",
      "step: 403, loss: 0.00020856146479632156\n",
      "step: 404, loss: 0.0002031693122355599\n",
      "step: 405, loss: 0.00019799184068127265\n",
      "step: 406, loss: 0.00019310138876039995\n",
      "step: 407, loss: 0.0001888007347239963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 408, loss: 0.00018406569165169487\n",
      "step: 409, loss: 0.0001790995343335447\n",
      "step: 410, loss: 0.00017431083454039556\n",
      "step: 411, loss: 0.00017092933508962294\n",
      "step: 412, loss: 0.00016668803457794285\n",
      "step: 413, loss: 0.00016254807320860692\n",
      "step: 414, loss: 0.00015842748927182362\n",
      "step: 415, loss: 0.00015539130098653556\n",
      "step: 416, loss: 0.00015207937386729764\n",
      "step: 417, loss: 0.0001485826352114368\n",
      "step: 418, loss: 0.000144928396111247\n",
      "step: 419, loss: 0.0001415188327201257\n",
      "step: 420, loss: 0.0001386868424060328\n",
      "step: 421, loss: 0.00013603302705075815\n",
      "step: 422, loss: 0.00013280042765970168\n",
      "step: 423, loss: 0.00013002682423117733\n",
      "step: 424, loss: 0.00012692382282437128\n",
      "step: 425, loss: 0.00012433125632048663\n",
      "step: 426, loss: 0.00012187766575927263\n",
      "step: 427, loss: 0.00011962163805845621\n",
      "step: 428, loss: 0.00011687347424223487\n",
      "step: 429, loss: 0.00011466791384343034\n",
      "step: 430, loss: 0.00011253789761694888\n",
      "step: 431, loss: 0.00011074808426424568\n",
      "step: 432, loss: 0.00010834410935067068\n",
      "step: 433, loss: 0.00010615044968201315\n",
      "step: 434, loss: 0.00010419453886763644\n",
      "step: 435, loss: 0.00010199549976827593\n",
      "step: 436, loss: 0.00010056829272615808\n",
      "step: 437, loss: 9.818414669565678e-05\n",
      "step: 438, loss: 9.683637675690637e-05\n",
      "step: 439, loss: 9.513882625751882e-05\n",
      "step: 440, loss: 9.318084565503384e-05\n",
      "step: 441, loss: 9.137816868525286e-05\n",
      "step: 442, loss: 8.983352825412197e-05\n",
      "step: 443, loss: 8.863197162575082e-05\n",
      "step: 444, loss: 8.709798182556308e-05\n",
      "step: 445, loss: 8.549704918560733e-05\n",
      "step: 446, loss: 8.396728436923032e-05\n",
      "step: 447, loss: 8.308109506656794e-05\n",
      "step: 448, loss: 8.180692465170614e-05\n",
      "step: 449, loss: 8.01231949675088e-05\n",
      "step: 450, loss: 7.894694805845107e-05\n",
      "step: 451, loss: 7.738797068689365e-05\n",
      "step: 452, loss: 7.595520964342323e-05\n",
      "step: 453, loss: 7.467475412736499e-05\n",
      "step: 454, loss: 7.344213423845364e-05\n",
      "step: 455, loss: 7.221852232319054e-05\n",
      "step: 456, loss: 7.135080787630185e-05\n",
      "step: 457, loss: 7.010586381108862e-05\n",
      "step: 458, loss: 6.925858036021322e-05\n",
      "step: 459, loss: 6.819850823149638e-05\n",
      "step: 460, loss: 6.701051387225307e-05\n",
      "step: 461, loss: 6.588706852157702e-05\n",
      "step: 462, loss: 6.505336094111969e-05\n",
      "step: 463, loss: 6.43564263975542e-05\n",
      "step: 464, loss: 6.338656961362918e-05\n",
      "step: 465, loss: 6.223680556267583e-05\n",
      "step: 466, loss: 6.105034510861529e-05\n",
      "step: 467, loss: 6.0082834161520604e-05\n",
      "step: 468, loss: 5.90424855994344e-05\n",
      "step: 469, loss: 5.810771057581168e-05\n",
      "step: 470, loss: 5.737621476414878e-05\n",
      "step: 471, loss: 5.683432678386252e-05\n",
      "step: 472, loss: 5.607925185265386e-05\n",
      "step: 473, loss: 5.539953846702816e-05\n",
      "step: 474, loss: 5.476719747840846e-05\n",
      "step: 475, loss: 5.38468695212152e-05\n",
      "step: 476, loss: 5.3049957478997455e-05\n",
      "step: 477, loss: 5.248239472541768e-05\n",
      "step: 478, loss: 5.1715825797462775e-05\n",
      "step: 479, loss: 5.1037769019046664e-05\n",
      "step: 480, loss: 5.050248578004901e-05\n",
      "step: 481, loss: 4.9976606859556175e-05\n",
      "step: 482, loss: 4.935488990064107e-05\n",
      "step: 483, loss: 4.8629536440969456e-05\n",
      "step: 484, loss: 4.79722912177967e-05\n",
      "step: 485, loss: 4.7132508978247646e-05\n",
      "step: 486, loss: 4.6695341643036153e-05\n",
      "step: 487, loss: 4.605803842957102e-05\n",
      "step: 488, loss: 4.559129892919844e-05\n",
      "step: 489, loss: 4.484544335235652e-05\n",
      "step: 490, loss: 4.4430135127466325e-05\n",
      "step: 491, loss: 4.387848606936685e-05\n",
      "step: 492, loss: 4.3340633553203174e-05\n",
      "step: 493, loss: 4.282429883703531e-05\n",
      "step: 494, loss: 4.246033266463653e-05\n",
      "step: 495, loss: 4.210829192877186e-05\n",
      "step: 496, loss: 4.1480148897002356e-05\n",
      "step: 497, loss: 4.109171845725901e-05\n",
      "step: 498, loss: 4.064692028092967e-05\n",
      "step: 499, loss: 4.009322364509696e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    # torch.mm（a, b）矩阵点乘操作\n",
    "    h = x.mm(w1)\n",
    "    # 实现relu，及取 x 和 0 之间的大值\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    # 计算loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print('step: {}, loss: {}'.format(i, loss))\n",
    "    \n",
    "    # 计算梯度，激活层的函数为relu\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # 权重更新\n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以上对pyTorch的Tensor操作有一个简单认识，如mm点乘操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyTorch有一个重要的对象，就是Variable，将Tensor转化为Variable后，pyTorch可以根据我们自定义的公式或者网络结构，实现自动梯度求导；\n",
    "\n",
    "<br\\>\n",
    "![variable](https://raw.githubusercontent.com/nanyoullm/nanyoullm.github.io/master/img/variable.png)\n",
    "<br\\>\n",
    "\n",
    "- 上图是Variable的重要属性。对于一个Variable对象，.data可以获得原始的tensor对象;当计算梯度后，该变量的梯度可以累计到，grad；\n",
    "- 我们先感受一下这个强大的功能，**很真实**；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在我们自定义一个$y$，$y=x+2$\n",
    "- 自定义一个$z$，$z=3*y^{2}$\n",
    "- $out=\\frac{1}{4}*z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "z = 3 * y * y\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 现在我们对out进行反向求导；\n",
    "- $out$对$x$的求导结果为：$\\frac{3}{2}*(x+2)$，$x$取矩阵中对应的值1，故为4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 请注意上面的结果，在out进行反向计算后，我们可以看到关系链最前段的x变量的梯度，但是我们无法看到y的梯度，返回了一个None，这是为什么呢？这里有一个合理的解释[pytorch_hook](https://www.zhihu.com/question/61044004)\n",
    "- pyTorch的开发者解释道：中间变量在完成了自身的反向传播使命后会被释放掉，因此我们想看中间变量的梯度，可以为其添加一个钩（hook)；直观理解，就是在这个中间变量完成反向传播计算的时候，再额外完成另一些任务，我们修改代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "[Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 用于记录y变量的梯度\n",
    "y_grads = []\n",
    "\n",
    "x = Variable(torch.ones(2,2), requires_grad=True)\n",
    "y = x + 2\n",
    "z = 3 * y * y\n",
    "out = z.mean()\n",
    "# 在反向传播之前，为y注册一个hook，任务是记录梯度\n",
    "y.register_hook(lambda grad: y_grads.append(grad))\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 有公式我们知道，x和y的梯度应该是一样的；\n",
    "- 同时我们再思考一下，这个hook还有什么用呢？\n",
    "> 当你训练了一个网络，想提取中间层的参数或者CNN中的feature map时，hook就可以排上用场啦！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 了解了Variable，现在我们使用Variable来重新实现上面的反向求导;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, input_size).type(torch.FloatTensor)\n",
    "x = Variable(x, requires_grad=False)\n",
    "y = torch.randn(batch_size, output_size).type(torch.FloatTensor)\n",
    "y = Variable(y, requires_grad=False)\n",
    "\n",
    "w1 = torch.randn(input_size, hidden_size).type(torch.FloatTensor)\n",
    "w1 = Variable(w1, requires_grad=True)\n",
    "w2 = torch.randn(hidden_size, output_size).type(torch.FloatTensor)\n",
    "w2 = Variable(w2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 31380328.0\n",
      "step: 1, loss: 27662854.0\n",
      "step: 2, loss: 28318600.0\n",
      "step: 3, loss: 32223158.0\n",
      "step: 4, loss: 38894172.0\n",
      "step: 5, loss: 48152132.0\n",
      "step: 6, loss: 59519988.0\n",
      "step: 7, loss: 72213096.0\n",
      "step: 8, loss: 84501200.0\n",
      "step: 9, loss: 94287640.0\n",
      "step: 10, loss: 98908912.0\n",
      "step: 11, loss: 96884448.0\n",
      "step: 12, loss: 88021744.0\n",
      "step: 13, loss: 74388632.0\n",
      "step: 14, loss: 58713200.0\n",
      "step: 15, loss: 43826708.0\n",
      "step: 16, loss: 31272546.0\n",
      "step: 17, loss: 21627858.0\n",
      "step: 18, loss: 14643799.0\n",
      "step: 19, loss: 9803993.0\n",
      "step: 20, loss: 6536228.0\n",
      "step: 21, loss: 4368750.5\n",
      "step: 22, loss: 2944001.5\n",
      "step: 23, loss: 2011553.5\n",
      "step: 24, loss: 1400832.125\n",
      "step: 25, loss: 999364.9375\n",
      "step: 26, loss: 733470.625\n",
      "step: 27, loss: 555392.4375\n",
      "step: 28, loss: 434254.0625\n",
      "step: 29, loss: 350252.78125\n",
      "step: 30, loss: 290586.375\n",
      "step: 31, loss: 247038.171875\n",
      "step: 32, loss: 214274.5625\n",
      "step: 33, loss: 188842.109375\n",
      "step: 34, loss: 168495.921875\n",
      "step: 35, loss: 151757.0625\n",
      "step: 36, loss: 137649.828125\n",
      "step: 37, loss: 125513.9765625\n",
      "step: 38, loss: 114899.5859375\n",
      "step: 39, loss: 105494.8671875\n",
      "step: 40, loss: 97081.3828125\n",
      "step: 41, loss: 89497.3125\n",
      "step: 42, loss: 82620.2421875\n",
      "step: 43, loss: 76356.0\n",
      "step: 44, loss: 70635.421875\n",
      "step: 45, loss: 65395.12109375\n",
      "step: 46, loss: 60584.25\n",
      "step: 47, loss: 56160.26171875\n",
      "step: 48, loss: 52087.80859375\n",
      "step: 49, loss: 48334.3828125\n",
      "step: 50, loss: 44871.39453125\n",
      "step: 51, loss: 41672.625\n",
      "step: 52, loss: 38718.16015625\n",
      "step: 53, loss: 35987.33984375\n",
      "step: 54, loss: 33459.70703125\n",
      "step: 55, loss: 31120.669921875\n",
      "step: 56, loss: 28955.9296875\n",
      "step: 57, loss: 26948.609375\n",
      "step: 58, loss: 25090.318359375\n",
      "step: 59, loss: 23367.009765625\n",
      "step: 60, loss: 21766.158203125\n",
      "step: 61, loss: 20282.99609375\n",
      "step: 62, loss: 18905.083984375\n",
      "step: 63, loss: 17625.34375\n",
      "step: 64, loss: 16435.955078125\n",
      "step: 65, loss: 15331.8349609375\n",
      "step: 66, loss: 14304.9072265625\n",
      "step: 67, loss: 13349.1904296875\n",
      "step: 68, loss: 12460.578125\n",
      "step: 69, loss: 11633.9482421875\n",
      "step: 70, loss: 10864.1337890625\n",
      "step: 71, loss: 10147.689453125\n",
      "step: 72, loss: 9480.177734375\n",
      "step: 73, loss: 8858.732421875\n",
      "step: 74, loss: 8279.2119140625\n",
      "step: 75, loss: 7739.5458984375\n",
      "step: 76, loss: 7236.3798828125\n",
      "step: 77, loss: 6767.05908203125\n",
      "step: 78, loss: 6329.60546875\n",
      "step: 79, loss: 5921.53369140625\n",
      "step: 80, loss: 5540.38134765625\n",
      "step: 81, loss: 5184.8779296875\n",
      "step: 82, loss: 4853.0703125\n",
      "step: 83, loss: 4542.90234375\n",
      "step: 84, loss: 4253.5478515625\n",
      "step: 85, loss: 3983.182373046875\n",
      "step: 86, loss: 3730.448486328125\n",
      "step: 87, loss: 3494.4296875\n",
      "step: 88, loss: 3273.743408203125\n",
      "step: 89, loss: 3067.435546875\n",
      "step: 90, loss: 2874.62451171875\n",
      "step: 91, loss: 2694.134521484375\n",
      "step: 92, loss: 2525.495849609375\n",
      "step: 93, loss: 2367.65185546875\n",
      "step: 94, loss: 2220.056884765625\n",
      "step: 95, loss: 2081.861572265625\n",
      "step: 96, loss: 1952.5218505859375\n",
      "step: 97, loss: 1831.5362548828125\n",
      "step: 98, loss: 1718.1768798828125\n",
      "step: 99, loss: 1612.074462890625\n",
      "step: 100, loss: 1512.734619140625\n",
      "step: 101, loss: 1419.6353759765625\n",
      "step: 102, loss: 1332.4739990234375\n",
      "step: 103, loss: 1250.781494140625\n",
      "step: 104, loss: 1174.2154541015625\n",
      "step: 105, loss: 1102.455322265625\n",
      "step: 106, loss: 1035.25244140625\n",
      "step: 107, loss: 972.2177734375\n",
      "step: 108, loss: 913.1311645507812\n",
      "step: 109, loss: 857.7232666015625\n",
      "step: 110, loss: 805.7666015625\n",
      "step: 111, loss: 757.0416259765625\n",
      "step: 112, loss: 711.3289794921875\n",
      "step: 113, loss: 668.4431762695312\n",
      "step: 114, loss: 628.2387084960938\n",
      "step: 115, loss: 590.4656982421875\n",
      "step: 116, loss: 555.0535888671875\n",
      "step: 117, loss: 521.81005859375\n",
      "step: 118, loss: 490.5922546386719\n",
      "step: 119, loss: 461.3068542480469\n",
      "step: 120, loss: 433.79437255859375\n",
      "step: 121, loss: 407.966064453125\n",
      "step: 122, loss: 383.69732666015625\n",
      "step: 123, loss: 360.93865966796875\n",
      "step: 124, loss: 339.5327453613281\n",
      "step: 125, loss: 319.4369201660156\n",
      "step: 126, loss: 300.5564270019531\n",
      "step: 127, loss: 282.8116149902344\n",
      "step: 128, loss: 266.1459655761719\n",
      "step: 129, loss: 250.47256469726562\n",
      "step: 130, loss: 235.75218200683594\n",
      "step: 131, loss: 221.9138946533203\n",
      "step: 132, loss: 208.9069061279297\n",
      "step: 133, loss: 196.6689453125\n",
      "step: 134, loss: 185.1757354736328\n",
      "step: 135, loss: 174.36212158203125\n",
      "step: 136, loss: 164.1951446533203\n",
      "step: 137, loss: 154.6274871826172\n",
      "step: 138, loss: 145.6392364501953\n",
      "step: 139, loss: 137.1735076904297\n",
      "step: 140, loss: 129.21685791015625\n",
      "step: 141, loss: 121.72895050048828\n",
      "step: 142, loss: 114.68538665771484\n",
      "step: 143, loss: 108.05722045898438\n",
      "step: 144, loss: 101.81981658935547\n",
      "step: 145, loss: 95.9477310180664\n",
      "step: 146, loss: 90.4244613647461\n",
      "step: 147, loss: 85.22164916992188\n",
      "step: 148, loss: 80.32799530029297\n",
      "step: 149, loss: 75.71784973144531\n",
      "step: 150, loss: 71.37779235839844\n",
      "step: 151, loss: 67.29439544677734\n",
      "step: 152, loss: 63.44719314575195\n",
      "step: 153, loss: 59.821434020996094\n",
      "step: 154, loss: 56.41011428833008\n",
      "step: 155, loss: 53.19806671142578\n",
      "step: 156, loss: 50.169471740722656\n",
      "step: 157, loss: 47.31787872314453\n",
      "step: 158, loss: 44.63071060180664\n",
      "step: 159, loss: 42.100975036621094\n",
      "step: 160, loss: 39.714107513427734\n",
      "step: 161, loss: 37.466575622558594\n",
      "step: 162, loss: 35.34844970703125\n",
      "step: 163, loss: 33.353553771972656\n",
      "step: 164, loss: 31.471635818481445\n",
      "step: 165, loss: 29.698659896850586\n",
      "step: 166, loss: 28.027368545532227\n",
      "step: 167, loss: 26.45142364501953\n",
      "step: 168, loss: 24.964874267578125\n",
      "step: 169, loss: 23.56539535522461\n",
      "step: 170, loss: 22.243824005126953\n",
      "step: 171, loss: 20.99820899963379\n",
      "step: 172, loss: 19.823131561279297\n",
      "step: 173, loss: 18.716419219970703\n",
      "step: 174, loss: 17.670881271362305\n",
      "step: 175, loss: 16.685701370239258\n",
      "step: 176, loss: 15.756016731262207\n",
      "step: 177, loss: 14.879263877868652\n",
      "step: 178, loss: 14.051785469055176\n",
      "step: 179, loss: 13.271015167236328\n",
      "step: 180, loss: 12.535264015197754\n",
      "step: 181, loss: 11.839706420898438\n",
      "step: 182, loss: 11.184229850769043\n",
      "step: 183, loss: 10.565665245056152\n",
      "step: 184, loss: 9.981104850769043\n",
      "step: 185, loss: 9.430074691772461\n",
      "step: 186, loss: 8.90969181060791\n",
      "step: 187, loss: 8.418327331542969\n",
      "step: 188, loss: 7.954765796661377\n",
      "step: 189, loss: 7.517103672027588\n",
      "step: 190, loss: 7.103961944580078\n",
      "step: 191, loss: 6.713597297668457\n",
      "step: 192, loss: 6.345378875732422\n",
      "step: 193, loss: 5.997286319732666\n",
      "step: 194, loss: 5.668718338012695\n",
      "step: 195, loss: 5.35830545425415\n",
      "step: 196, loss: 5.06500244140625\n",
      "step: 197, loss: 4.7884602546691895\n",
      "step: 198, loss: 4.527106285095215\n",
      "step: 199, loss: 4.279994487762451\n",
      "step: 200, loss: 4.046718597412109\n",
      "step: 201, loss: 3.8262405395507812\n",
      "step: 202, loss: 3.6183056831359863\n",
      "step: 203, loss: 3.421576499938965\n",
      "step: 204, loss: 3.235534906387329\n",
      "step: 205, loss: 3.0599727630615234\n",
      "step: 206, loss: 2.8939766883850098\n",
      "step: 207, loss: 2.7371914386749268\n",
      "step: 208, loss: 2.588905096054077\n",
      "step: 209, loss: 2.4489424228668213\n",
      "step: 210, loss: 2.3164126873016357\n",
      "step: 211, loss: 2.1913349628448486\n",
      "step: 212, loss: 2.072993278503418\n",
      "step: 213, loss: 1.961235523223877\n",
      "step: 214, loss: 1.855592131614685\n",
      "step: 215, loss: 1.7555803060531616\n",
      "step: 216, loss: 1.661255121231079\n",
      "step: 217, loss: 1.5719170570373535\n",
      "step: 218, loss: 1.4873404502868652\n",
      "step: 219, loss: 1.4074525833129883\n",
      "step: 220, loss: 1.3321141004562378\n",
      "step: 221, loss: 1.2606401443481445\n",
      "step: 222, loss: 1.193047285079956\n",
      "step: 223, loss: 1.129331111907959\n",
      "step: 224, loss: 1.0688611268997192\n",
      "step: 225, loss: 1.0116729736328125\n",
      "step: 226, loss: 0.9576861262321472\n",
      "step: 227, loss: 0.9064664244651794\n",
      "step: 228, loss: 0.8582375645637512\n",
      "step: 229, loss: 0.8124324083328247\n",
      "step: 230, loss: 0.7691198587417603\n",
      "step: 231, loss: 0.7282100319862366\n",
      "step: 232, loss: 0.6893944144248962\n",
      "step: 233, loss: 0.6529227495193481\n",
      "step: 234, loss: 0.6182078123092651\n",
      "step: 235, loss: 0.5853461623191833\n",
      "step: 236, loss: 0.5543578863143921\n",
      "step: 237, loss: 0.5249153971672058\n",
      "step: 238, loss: 0.49708452820777893\n",
      "step: 239, loss: 0.47082704305648804\n",
      "step: 240, loss: 0.44585689902305603\n",
      "step: 241, loss: 0.42225155234336853\n",
      "step: 242, loss: 0.39998120069503784\n",
      "step: 243, loss: 0.37888529896736145\n",
      "step: 244, loss: 0.3588777482509613\n",
      "step: 245, loss: 0.33992981910705566\n",
      "step: 246, loss: 0.3219764828681946\n",
      "step: 247, loss: 0.3050672709941864\n",
      "step: 248, loss: 0.28891709446907043\n",
      "step: 249, loss: 0.27375057339668274\n",
      "step: 250, loss: 0.2593865394592285\n",
      "step: 251, loss: 0.24571478366851807\n",
      "step: 252, loss: 0.23280282318592072\n",
      "step: 253, loss: 0.22066588699817657\n",
      "step: 254, loss: 0.20906378328800201\n",
      "step: 255, loss: 0.1980574131011963\n",
      "step: 256, loss: 0.18769820034503937\n",
      "step: 257, loss: 0.1778469979763031\n",
      "step: 258, loss: 0.168562114238739\n",
      "step: 259, loss: 0.15977992117404938\n",
      "step: 260, loss: 0.15138356387615204\n",
      "step: 261, loss: 0.14348219335079193\n",
      "step: 262, loss: 0.13600870966911316\n",
      "step: 263, loss: 0.1288774162530899\n",
      "step: 264, loss: 0.12217754125595093\n",
      "step: 265, loss: 0.11580067127943039\n",
      "step: 266, loss: 0.10978110879659653\n",
      "step: 267, loss: 0.10404836386442184\n",
      "step: 268, loss: 0.09863535314798355\n",
      "step: 269, loss: 0.09351607412099838\n",
      "step: 270, loss: 0.08864405006170273\n",
      "step: 271, loss: 0.0840323194861412\n",
      "step: 272, loss: 0.07963988184928894\n",
      "step: 273, loss: 0.0755447968840599\n",
      "step: 274, loss: 0.07160961627960205\n",
      "step: 275, loss: 0.06793803721666336\n",
      "step: 276, loss: 0.06443312764167786\n",
      "step: 277, loss: 0.06105034425854683\n",
      "step: 278, loss: 0.057892151176929474\n",
      "step: 279, loss: 0.054898761212825775\n",
      "step: 280, loss: 0.052103858441114426\n",
      "step: 281, loss: 0.0493745394051075\n",
      "step: 282, loss: 0.04682893678545952\n",
      "step: 283, loss: 0.04443001374602318\n",
      "step: 284, loss: 0.042103420943021774\n",
      "step: 285, loss: 0.03993910551071167\n",
      "step: 286, loss: 0.03786744922399521\n",
      "step: 287, loss: 0.035931214690208435\n",
      "step: 288, loss: 0.034073103219270706\n",
      "step: 289, loss: 0.03233858197927475\n",
      "step: 290, loss: 0.03066934086382389\n",
      "step: 291, loss: 0.029094494879245758\n",
      "step: 292, loss: 0.027606219053268433\n",
      "step: 293, loss: 0.026187730953097343\n",
      "step: 294, loss: 0.024847950786352158\n",
      "step: 295, loss: 0.02356693521142006\n",
      "step: 296, loss: 0.022362766787409782\n",
      "step: 297, loss: 0.021220063790678978\n",
      "step: 298, loss: 0.020138584077358246\n",
      "step: 299, loss: 0.019096333533525467\n",
      "step: 300, loss: 0.01811486855149269\n",
      "step: 301, loss: 0.017180779948830605\n",
      "step: 302, loss: 0.016301609575748444\n",
      "step: 303, loss: 0.01548665203154087\n",
      "step: 304, loss: 0.014687958173453808\n",
      "step: 305, loss: 0.013938874937593937\n",
      "step: 306, loss: 0.013230538927018642\n",
      "step: 307, loss: 0.012577819637954235\n",
      "step: 308, loss: 0.011935191228985786\n",
      "step: 309, loss: 0.01133816223591566\n",
      "step: 310, loss: 0.010760231874883175\n",
      "step: 311, loss: 0.010212779976427555\n",
      "step: 312, loss: 0.009691722691059113\n",
      "step: 313, loss: 0.009209764190018177\n",
      "step: 314, loss: 0.008741063065826893\n",
      "step: 315, loss: 0.008296574465930462\n",
      "step: 316, loss: 0.00788530707359314\n",
      "step: 317, loss: 0.007488934323191643\n",
      "step: 318, loss: 0.007112749386578798\n",
      "step: 319, loss: 0.006756839342415333\n",
      "step: 320, loss: 0.006423094775527716\n",
      "step: 321, loss: 0.006100562401115894\n",
      "step: 322, loss: 0.005797578953206539\n",
      "step: 323, loss: 0.005516383331269026\n",
      "step: 324, loss: 0.005249569658190012\n",
      "step: 325, loss: 0.004985827021300793\n",
      "step: 326, loss: 0.004739720840007067\n",
      "step: 327, loss: 0.004508202895522118\n",
      "step: 328, loss: 0.004288188647478819\n",
      "step: 329, loss: 0.004080291371792555\n",
      "step: 330, loss: 0.003885259386152029\n",
      "step: 331, loss: 0.0036940034478902817\n",
      "step: 332, loss: 0.0035156947560608387\n",
      "step: 333, loss: 0.0033494974486529827\n",
      "step: 334, loss: 0.0031912901904433966\n",
      "step: 335, loss: 0.0030349690932780504\n",
      "step: 336, loss: 0.002895522862672806\n",
      "step: 337, loss: 0.0027552214451134205\n",
      "step: 338, loss: 0.00262730591930449\n",
      "step: 339, loss: 0.0025077289901673794\n",
      "step: 340, loss: 0.0023926522117108107\n",
      "step: 341, loss: 0.0022831291425973177\n",
      "step: 342, loss: 0.002174846827983856\n",
      "step: 343, loss: 0.002074186922982335\n",
      "step: 344, loss: 0.001982650486752391\n",
      "step: 345, loss: 0.001892631407827139\n",
      "step: 346, loss: 0.0018101203022524714\n",
      "step: 347, loss: 0.0017287700902670622\n",
      "step: 348, loss: 0.0016555492766201496\n",
      "step: 349, loss: 0.0015840366249904037\n",
      "step: 350, loss: 0.0015170612605288625\n",
      "step: 351, loss: 0.0014538480900228024\n",
      "step: 352, loss: 0.001387846888974309\n",
      "step: 353, loss: 0.0013279690174385905\n",
      "step: 354, loss: 0.0012712308671325445\n",
      "step: 355, loss: 0.00121969950851053\n",
      "step: 356, loss: 0.0011693971464410424\n",
      "step: 357, loss: 0.0011218294966965914\n",
      "step: 358, loss: 0.0010726528707891703\n",
      "step: 359, loss: 0.0010298837441951036\n",
      "step: 360, loss: 0.0009891471127048135\n",
      "step: 361, loss: 0.0009511394891887903\n",
      "step: 362, loss: 0.0009121779003180563\n",
      "step: 363, loss: 0.0008760418859310448\n",
      "step: 364, loss: 0.0008409128640778363\n",
      "step: 365, loss: 0.0008071983465924859\n",
      "step: 366, loss: 0.0007765641203150153\n",
      "step: 367, loss: 0.0007493505836464465\n",
      "step: 368, loss: 0.000719101051799953\n",
      "step: 369, loss: 0.000693763024173677\n",
      "step: 370, loss: 0.0006664081593044102\n",
      "step: 371, loss: 0.0006416780524887145\n",
      "step: 372, loss: 0.0006193394656293094\n",
      "step: 373, loss: 0.000596225552726537\n",
      "step: 374, loss: 0.0005755489110015333\n",
      "step: 375, loss: 0.0005554812378250062\n",
      "step: 376, loss: 0.0005377740599215031\n",
      "step: 377, loss: 0.0005194434197619557\n",
      "step: 378, loss: 0.0005006742430850863\n",
      "step: 379, loss: 0.0004839193425141275\n",
      "step: 380, loss: 0.00046877790009602904\n",
      "step: 381, loss: 0.0004536945780273527\n",
      "step: 382, loss: 0.0004379349120426923\n",
      "step: 383, loss: 0.0004243671428412199\n",
      "step: 384, loss: 0.0004114033072255552\n",
      "step: 385, loss: 0.00039801004459150136\n",
      "step: 386, loss: 0.00038518186192959547\n",
      "step: 387, loss: 0.00037407258059829473\n",
      "step: 388, loss: 0.0003631474683061242\n",
      "step: 389, loss: 0.00035237896372564137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 390, loss: 0.00034263409906998277\n",
      "step: 391, loss: 0.0003325456054881215\n",
      "step: 392, loss: 0.0003236168413423002\n",
      "step: 393, loss: 0.000315542274620384\n",
      "step: 394, loss: 0.0003058273287024349\n",
      "step: 395, loss: 0.0002973714435938746\n",
      "step: 396, loss: 0.0002901450789067894\n",
      "step: 397, loss: 0.000281019601970911\n",
      "step: 398, loss: 0.0002729268744587898\n",
      "step: 399, loss: 0.00026575347874313593\n",
      "step: 400, loss: 0.0002589224313851446\n",
      "step: 401, loss: 0.00025108037516474724\n",
      "step: 402, loss: 0.00024512867094017565\n",
      "step: 403, loss: 0.00023825983225833625\n",
      "step: 404, loss: 0.00023214535031002015\n",
      "step: 405, loss: 0.0002258922322653234\n",
      "step: 406, loss: 0.00022039844770915806\n",
      "step: 407, loss: 0.0002148994244635105\n",
      "step: 408, loss: 0.0002099351113429293\n",
      "step: 409, loss: 0.00020412332378327847\n",
      "step: 410, loss: 0.00019952727598138154\n",
      "step: 411, loss: 0.00019556129700504243\n",
      "step: 412, loss: 0.00019042038184124976\n",
      "step: 413, loss: 0.0001861666387412697\n",
      "step: 414, loss: 0.0001815163268474862\n",
      "step: 415, loss: 0.00017777220637071878\n",
      "step: 416, loss: 0.00017378490883857012\n",
      "step: 417, loss: 0.0001700668508419767\n",
      "step: 418, loss: 0.00016648847667966038\n",
      "step: 419, loss: 0.00016263709403574467\n",
      "step: 420, loss: 0.00015884039748925716\n",
      "step: 421, loss: 0.00015581969637423754\n",
      "step: 422, loss: 0.00015209852426778525\n",
      "step: 423, loss: 0.00014885129348840564\n",
      "step: 424, loss: 0.00014608535275328904\n",
      "step: 425, loss: 0.00014326556993182749\n",
      "step: 426, loss: 0.0001399658649461344\n",
      "step: 427, loss: 0.00013685457815881819\n",
      "step: 428, loss: 0.0001343316544080153\n",
      "step: 429, loss: 0.00013136776397004724\n",
      "step: 430, loss: 0.00012865549069829285\n",
      "step: 431, loss: 0.0001265659084310755\n",
      "step: 432, loss: 0.00012446109030861408\n",
      "step: 433, loss: 0.00012239665375091136\n",
      "step: 434, loss: 0.00012000977585557848\n",
      "step: 435, loss: 0.00011740637273760512\n",
      "step: 436, loss: 0.00011488289601402357\n",
      "step: 437, loss: 0.00011301764607196674\n",
      "step: 438, loss: 0.00011079236719524488\n",
      "step: 439, loss: 0.00010880985064432025\n",
      "step: 440, loss: 0.00010681958519853652\n",
      "step: 441, loss: 0.00010446691157994792\n",
      "step: 442, loss: 0.00010212658526143059\n",
      "step: 443, loss: 0.00010044074588222429\n",
      "step: 444, loss: 9.896280243992805e-05\n",
      "step: 445, loss: 9.761568799149245e-05\n",
      "step: 446, loss: 9.592113929102197e-05\n",
      "step: 447, loss: 9.434485400561243e-05\n",
      "step: 448, loss: 9.281794336857274e-05\n",
      "step: 449, loss: 9.14112024474889e-05\n",
      "step: 450, loss: 8.952464850153774e-05\n",
      "step: 451, loss: 8.810234430711716e-05\n",
      "step: 452, loss: 8.657336002215743e-05\n",
      "step: 453, loss: 8.540366252418607e-05\n",
      "step: 454, loss: 8.374750905204564e-05\n",
      "step: 455, loss: 8.246219658758491e-05\n",
      "step: 456, loss: 8.085112494882196e-05\n",
      "step: 457, loss: 7.973537140060216e-05\n",
      "step: 458, loss: 7.857422315282747e-05\n",
      "step: 459, loss: 7.72683706600219e-05\n",
      "step: 460, loss: 7.593912596348673e-05\n",
      "step: 461, loss: 7.49524260754697e-05\n",
      "step: 462, loss: 7.358564471360296e-05\n",
      "step: 463, loss: 7.254668162204325e-05\n",
      "step: 464, loss: 7.123246177798137e-05\n",
      "step: 465, loss: 7.035411545075476e-05\n",
      "step: 466, loss: 6.931966345291585e-05\n",
      "step: 467, loss: 6.819394911872223e-05\n",
      "step: 468, loss: 6.735747592756525e-05\n",
      "step: 469, loss: 6.62986421957612e-05\n",
      "step: 470, loss: 6.528289668494835e-05\n",
      "step: 471, loss: 6.416482210624963e-05\n",
      "step: 472, loss: 6.336563092190772e-05\n",
      "step: 473, loss: 6.25292377662845e-05\n",
      "step: 474, loss: 6.159342592582107e-05\n",
      "step: 475, loss: 6.08440677751787e-05\n",
      "step: 476, loss: 6.0279882745817304e-05\n",
      "step: 477, loss: 5.9389429225120693e-05\n",
      "step: 478, loss: 5.833884279127233e-05\n",
      "step: 479, loss: 5.7724602811504155e-05\n",
      "step: 480, loss: 5.7020803069463e-05\n",
      "step: 481, loss: 5.6052511354209855e-05\n",
      "step: 482, loss: 5.5644417443545535e-05\n",
      "step: 483, loss: 5.470693213283084e-05\n",
      "step: 484, loss: 5.4097890824778005e-05\n",
      "step: 485, loss: 5.309069820214063e-05\n",
      "step: 486, loss: 5.255545329418965e-05\n",
      "step: 487, loss: 5.18049746460747e-05\n",
      "step: 488, loss: 5.1094077207380906e-05\n",
      "step: 489, loss: 5.036436778027564e-05\n",
      "step: 490, loss: 4.973545219399966e-05\n",
      "step: 491, loss: 4.9145535740535706e-05\n",
      "step: 492, loss: 4.887762406724505e-05\n",
      "step: 493, loss: 4.813132545677945e-05\n",
      "step: 494, loss: 4.764495315612294e-05\n",
      "step: 495, loss: 4.6922898036427796e-05\n",
      "step: 496, loss: 4.652907227864489e-05\n",
      "step: 497, loss: 4.6091488911770284e-05\n",
      "step: 498, loss: 4.558794171316549e-05\n",
      "step: 499, loss: 4.501720832195133e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print('step: {}, loss: {}'.format(i, loss.data[0]))\n",
    "    \n",
    "    loss.backward()\n",
    "    w1.data = w1.data - learning_rate * w1.grad.data\n",
    "    w2.data = w2.data - learning_rate * w2.grad.data\n",
    "    \n",
    "    # 注意！！！ 前面我们说到，Variable的grad是会累计的，所以每次计算之后需要清零\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前面我们说到，Variable的grad是会累计的，所以每次计算之后需要清零;\n",
    "- 另外Variable.data可以获取到tensor对象;\n",
    "- 通过自动计算梯度，我们可以免去自行推公式的繁琐，并且保证不会出错;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor VS Numpy\n",
    "pyTorch内设置了tensor和numpy的array的转换桥梁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# array to tensor\n",
    "a = torch.from_numpy(np.array([1, 2, 3]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# tensor to array\n",
    "b = torch.ones(2, 2).numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义autograd函数\n",
    "- pytorch里，用户可以自定义autograd函数，可以参考[autograd](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-defining-new-autograd-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enjoy it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}